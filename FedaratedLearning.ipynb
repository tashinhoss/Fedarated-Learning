{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kBWigvlnemL9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### From Github"
      ],
      "metadata": {
        "id": "kBWigvlnemL9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S8nhNIGqUVG",
        "outputId": "b9198022-fd92-41dc-8b19-fc5b156fd398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Federated-Learning-PyTorch'...\n",
            "remote: Enumerating objects: 261, done.\u001b[K\n",
            "remote: Total 261 (delta 0), reused 0 (delta 0), pack-reused 261\u001b[K\n",
            "Receiving objects: 100% (261/261), 214.52 KiB | 4.88 MiB/s, done.\n",
            "Resolving deltas: 100% (136/136), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AshwinRJ/Federated-Learning-PyTorch.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python '/content/Federated-Learning-PyTorch/src/baseline_main.py' --model=mlp --dataset=mnist --epochs=2"
      ],
      "metadata": {
        "id": "tpzseoFNqavP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorboardX"
      ],
      "metadata": {
        "id": "s6-l-GzqsVDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python '/content/Federated-Learning-PyTorch/src/federated_main.py' --model=cnn --dataset=cifar --gpu=0 --iid=1 --epochs=2"
      ],
      "metadata": {
        "id": "bQ4rebzKsIEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "EwRis4wLk7Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "71qIkFaL-lNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "a1 = 0.0\n",
        "a2 = 0.0\n",
        "\n",
        "H = 1.0\n",
        "M = 0.5\n",
        "L = 0.0\n",
        "\n",
        "H1 = 0.0\n",
        "M1 = 0.0\n",
        "L1 = 0.0\n",
        "\n",
        "H2 = 0.0\n",
        "M2 = 0.0\n",
        "L2 = 0.0\n",
        "\n",
        "numberOfAntAttributes = 2\n",
        "matchingDegree = [0.0] * 9\n",
        "relativeWeight = 1.0\n",
        "totalWeight = 0.0\n",
        "consequentBeliefDegree = [0.0] * 27\n",
        "updatedConsequentBeliefDegree = [0.0] * 27\n",
        "beliefDegreeChangeLevel = 0.0\n",
        "activationWeight = [0.0] * 9\n",
        "ruleWiseBeliefDegreeSum = [0.0] * 9\n",
        "\n",
        "def transformInput1(i):\n",
        "    global H1, M1, L1\n",
        "    if i >= H:\n",
        "        H1 = 1.0\n",
        "        M1 = 0.0\n",
        "        L1 = 0.0\n",
        "    elif i == M:\n",
        "        H1 = 0.0\n",
        "        M1 = 1.0\n",
        "        L1 = 0.0\n",
        "    elif i <= L:\n",
        "        H1 = 0.0\n",
        "        M1 = 0.0\n",
        "        L1 = 1.0\n",
        "    elif (i <= H) and (i >= M):\n",
        "        M1 = (H - i) / (H - M)\n",
        "        H1 = 1.0 - M1\n",
        "        L1 = 0.0\n",
        "    elif (i <= M) and (i >= L):\n",
        "        L1 = (M - i) / (M - L)\n",
        "        M1 = 1.0 - L1\n",
        "        H1 = 0.0\n",
        "\n",
        "def transformInput2(i):\n",
        "    global H2, M2, L2\n",
        "    if i >= H:\n",
        "        H2 = 1.0\n",
        "        M2 = 0.0\n",
        "        L2 = 0.0\n",
        "    elif i == M:\n",
        "        H2 = 0.0\n",
        "        M2 = 1.0\n",
        "        L2 = 0.0\n",
        "    elif i <= L:\n",
        "        H2 = 0.0\n",
        "        M2 = 0.0\n",
        "        L2 = 1.0\n",
        "    elif (i <= H) and (i >= M):\n",
        "        M2 = (H - i) / (H - M)\n",
        "        H2 = 1.0 - M2\n",
        "        L2 = 0.0\n",
        "    elif (i <= M) and (i >= L):\n",
        "        L2 = (M - i) / (M - L)\n",
        "        M2 = 1.0 - L2\n",
        "        H2 = 0.0\n",
        "\n",
        "def ruleBase():\n",
        "    global consequentBeliefDegree\n",
        "    consequentBeliefDegree[0] = 1.0\n",
        "    consequentBeliefDegree[1] = 0.0\n",
        "    consequentBeliefDegree[2] = 0.0\n",
        "    consequentBeliefDegree[3] = 0.0\n",
        "    consequentBeliefDegree[4] = 0.40\n",
        "    consequentBeliefDegree[5] = 0.60\n",
        "    consequentBeliefDegree[6] = 0.0\n",
        "    consequentBeliefDegree[7] = 0.0\n",
        "    consequentBeliefDegree[8] = 1.0\n",
        "    consequentBeliefDegree[9] = 1.0\n",
        "    consequentBeliefDegree[10] = 0.0\n",
        "    consequentBeliefDegree[11] = 0.0\n",
        "    consequentBeliefDegree[12] = 0.0\n",
        "    consequentBeliefDegree[13] = 1.0\n",
        "    consequentBeliefDegree[14] = 0.0\n",
        "    consequentBeliefDegree[15] = 0.0\n",
        "    consequentBeliefDegree[16] = 0.40\n",
        "    consequentBeliefDegree[17] = 0.60\n",
        "    consequentBeliefDegree[18] = 0.0\n",
        "    consequentBeliefDegree[19] = 0.0\n",
        "    consequentBeliefDegree[20] = 1.0\n",
        "    consequentBeliefDegree[21] = 0.60\n",
        "    consequentBeliefDegree[22] = 0.40\n",
        "    consequentBeliefDegree[23] = 0.0\n",
        "    consequentBeliefDegree[24] = 0.0\n",
        "    consequentBeliefDegree[25] = 0.0\n",
        "    consequentBeliefDegree[26] = 1.0\n",
        "\n",
        "def takeInput():\n",
        "    global a1, a2\n",
        "    print(\"Insert value for A1 (between 0 and 1): \")\n",
        "    a1 = float(input())\n",
        "    print(\"Insert value for A2 (between 0 and 1): \")\n",
        "    a2 = float(input())\n",
        "    transformInput1(a1)\n",
        "    transformInput2(a2)\n",
        "\n",
        "\n",
        "def show_transformed_input():\n",
        "    print(f\"\\nTransformed Input is as follow.\")\n",
        "    print(f\"A1 = {{(H, {H1}); (M, {M1}); (L, {L1})}}\")\n",
        "    print(f\"A2 = {{(H, {H2}); (M, {M2}); (L, {L2})}}\")\n",
        "\n",
        "def calculate_matching_degree():\n",
        "    increment = 0\n",
        "    ti1 = [H1, M1, L1]\n",
        "    ti2 = [H2, M2, L2]\n",
        "\n",
        "    for c in range(3):\n",
        "        for d in range(3):\n",
        "            matchingDegree[increment] = pow(ti1[c], relativeWeight) * pow(ti2[d], relativeWeight)\n",
        "            increment += 1\n",
        "\n",
        "def show_matching_degree():\n",
        "    print(f\"\\nMatching degrees of the rules are as follow.\")\n",
        "    for counter in range(9):\n",
        "        print(f\"Matching Degree of Rule {counter+1} = {matchingDegree[counter]}\")\n",
        "\n",
        "def show_activation_weight():\n",
        "    total_weight = 0\n",
        "    for x in range(9):\n",
        "        total_weight += matchingDegree[x]\n",
        "\n",
        "    print(f\"\\nActivation Weights of the rules are as follow.\")\n",
        "    for counter in range(9):\n",
        "        activationWeight[counter] = matchingDegree[counter] / total_weight\n",
        "        print(f\"Activation weight of Rule {counter+1} = {activationWeight[counter]}\")\n",
        "\n",
        "\n",
        "def update_belief_degree():\n",
        "    update = 0\n",
        "    sum_ant_attr1 = 1\n",
        "    sum_ant_attr2 = 1\n",
        "\n",
        "    if (H1 + M1 + L1) < 1:\n",
        "        sum_ant_attr1 = H1 + M1 + L1\n",
        "        update = 1\n",
        "\n",
        "    if (H2 + M2 + L2) < 1:\n",
        "        sum_ant_attr2 = H2 + M2 + L2\n",
        "        update = 1\n",
        "\n",
        "    if update == 1:\n",
        "        belief_degree_change_level = (sum_ant_attr1 + sum_ant_attr2) / numberOfAntAttributes\n",
        "        print(f\"Belief Degree Change Level = {belief_degree_change_level}\")\n",
        "        for go in range(27):\n",
        "            consequentBeliefDegree[go] = belief_degree_change_level * consequentBeliefDegree[go]\n",
        "            print(f\"Updated Consequent Belief Degree : {consequentBeliefDegree[go]}\")\n",
        "    else:\n",
        "        print(\"\\nNo upgradation of belief degree required.\")\n",
        "\n",
        "\n",
        "def aggregateER():\n",
        "    parse = 0\n",
        "    move1 = 0\n",
        "    move2 = 1\n",
        "    move3 = 2\n",
        "    action1 = 0\n",
        "    action2 = 1\n",
        "    action3 = 2\n",
        "\n",
        "    part11 = 1\n",
        "    part12 = 1\n",
        "    part13 = 1\n",
        "    flpart1 = 1\n",
        "    part2 = 1.0\n",
        "    value = 1.0\n",
        "    meu = 1.0\n",
        "\n",
        "    numeratorH1 = 1.0\n",
        "    numeratorH2 = 1.0\n",
        "    numeratorH = 1.0\n",
        "    denominatorH1 = 1.0\n",
        "    denominatorH = 1.0\n",
        "    aggregatedBeliefDegreeH = 1.0\n",
        "    numeratorM1 = 1.0\n",
        "    numeratorM = 1.0\n",
        "    aggregatedBeliefDegreeM = 1.0\n",
        "    numeratorL1 = 1.0\n",
        "    numeratorL = 1.0\n",
        "    aggregatedBeliefDegreeL = 1.0\n",
        "    utilityScoreH = 1.0\n",
        "    utilityScoreM = 0.5\n",
        "    utilityScoreL = 0.0\n",
        "    crispValue = 1.0\n",
        "    degreeOfIncompleteness = 1.0\n",
        "    utilityMax = 1.0\n",
        "    utilityMin = 1.0\n",
        "    utilityAvg = 1.0\n",
        "\n",
        "    for t in range(9):\n",
        "        parse = t * 3\n",
        "        ruleWiseBeliefDegreeSum[t] = (consequentBeliefDegree[parse] +consequentBeliefDegree[parse + 1] + consequentBeliefDegree[parse + 2])\n",
        "\n",
        "\n",
        "    for rule in range(9):\n",
        "        part11 *= (activationWeight[rule] * consequentBeliefDegree[move1] + 1 - (activationWeight[rule] * ruleWiseBeliefDegreeSum[rule]))\n",
        "        move1 += 3\n",
        "\n",
        "\n",
        "    for rule in range(9):\n",
        "        part12 *= (activationWeight[rule] * consequentBeliefDegree[move2] + 1 - (activationWeight[rule] * ruleWiseBeliefDegreeSum[rule]))\n",
        "        move2 += 3\n",
        "\n",
        "\n",
        "    for rule in range(9):\n",
        "        part13 *= (activationWeight[rule] * consequentBeliefDegree[move3] + 1 - (activationWeight[rule] * ruleWiseBeliefDegreeSum[rule]))\n",
        "        move3 += 3\n",
        "\n",
        "\n",
        "    part1 = (part11 + part12 + part13)\n",
        "\n",
        "\n",
        "    for rule in range(9):\n",
        "        part2 *= (1 - (activationWeight[rule] * ruleWiseBeliefDegreeSum[rule]))\n",
        "\n",
        "\n",
        "    value = part1 - part2\n",
        "\n",
        "    meu = 1/value\n",
        "\n",
        "\n",
        "    for rule in range(9):\n",
        "        numeratorH1 *= (activationWeight[rule] * consequentBeliefDegree[action1] + 1 - (activationWeight[rule] * ruleWiseBeliefDegreeSum[rule]))\n",
        "        action1 += 3\n",
        "\n",
        "\n",
        "    for rule in range(9):\n",
        "        numeratorH2 *= (1 - (activationWeight[rule] * ruleWiseBeliefDegreeSum[rule]))\n",
        "\n",
        "\n",
        "    numeratorH = meu * (numeratorH1 - numeratorH2)\n",
        "\n",
        "\n",
        "    for rule in range(9):\n",
        "        denominatorH1 *= (1 - activationWeight[rule])\n",
        "\n",
        "    denominatorH = 1 - (meu * denominatorH1)\n",
        "    aggregatedBeliefDegreeH = numeratorH / denominatorH\n",
        "    print(\"ER Aggregated Belief Degree for High:\", aggregatedBeliefDegreeH)\n",
        "\n",
        "    for rule in range(9):\n",
        "        numeratorM1 *= (activationWeight[rule] * consequentBeliefDegree[action2] + 1 - (activationWeight[rule] * ruleWiseBeliefDegreeSum[rule]))\n",
        "        action2 += 3\n",
        "\n",
        "\n",
        "    numeratorM = meu * (numeratorM1 - numeratorH2)\n",
        "    aggregatedBeliefDegreeM = (numeratorM/denominatorH)\n",
        "    print(f\"ER Aggregated Belief Degree for Medium: {aggregatedBeliefDegreeM}\")\n",
        "\n",
        "    for rule in range(9):\n",
        "        numeratorL1 *= (activationWeight[rule] * consequentBeliefDegree[action3] + 1 - (activationWeight[rule] * ruleWiseBeliefDegreeSum[rule]))\n",
        "        action3 += 3\n",
        "\n",
        "    numeratorL = meu * (numeratorL1 - numeratorH2)\n",
        "    aggregatedBeliefDegreeL = numeratorL / denominatorH\n",
        "    print(\"ER Aggregated Belief Degree for Low:\", aggregatedBeliefDegreeL)\n",
        "\n",
        "    if (aggregatedBeliefDegreeH + aggregatedBeliefDegreeM + aggregatedBeliefDegreeL) == 1:\n",
        "        crispValue = (aggregatedBeliefDegreeH * utilityScoreH) + (aggregatedBeliefDegreeM * utilityScoreM) + (aggregatedBeliefDegreeL * utilityScoreL)\n",
        "        print(\"Crisp or numerical value is:\", crispValue)\n",
        "\n",
        "    else:\n",
        "        degreeOfIncompleteness = 1 - (aggregatedBeliefDegreeH + aggregatedBeliefDegreeM + aggregatedBeliefDegreeL)\n",
        "        print(\"Unassigned Degree of Belief:\", degreeOfIncompleteness)\n",
        "\n",
        "    utilityMax = ((aggregatedBeliefDegreeH + degreeOfIncompleteness) * utilityScoreH + (aggregatedBeliefDegreeM * utilityScoreM) + (aggregatedBeliefDegreeL * utilityScoreL))\n",
        "\n",
        "    utilityMin = (aggregatedBeliefDegreeH * utilityScoreH) + (aggregatedBeliefDegreeM * utilityScoreM) + (aggregatedBeliefDegreeL + degreeOfIncompleteness) * utilityScoreL\n",
        "\n",
        "    utilityAvg = (utilityMax + utilityMin) / 2\n",
        "\n",
        "    print(\"Maximum expected utility:\", utilityMax)\n",
        "    print(\"Minimum expected utility:\", utilityMin)\n",
        "    print(\"Average expected utility:\", utilityAvg)\n",
        "\n",
        "    print(\"Revised ER Aggregated Belief Degrees considering degree of Incompleteness are as follows:\")\n",
        "    print(\"Revised ER Aggregated Belief Degree for High:\", aggregatedBeliefDegreeH / (aggregatedBeliefDegreeH + aggregatedBeliefDegreeM + aggregatedBeliefDegreeL))\n",
        "    print(\"Revised ER Aggregated Belief Degree for Medium:\", aggregatedBeliefDegreeM / (aggregatedBeliefDegreeH + aggregatedBeliefDegreeM + aggregatedBeliefDegreeL))\n",
        "    print(\"Revised ER Aggregated Belief Degree for Low:\", aggregatedBeliefDegreeL / (aggregatedBeliefDegreeH + aggregatedBeliefDegreeM + aggregatedBeliefDegreeL))\n",
        "\n",
        "def main():\n",
        "    ruleBase()\n",
        "    takeInput()\n",
        "    show_transformed_input()\n",
        "    calculate_matching_degree()\n",
        "    show_matching_degree()\n",
        "    show_activation_weight()\n",
        "    update_belief_degree()\n",
        "    aggregateER()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "AI6bwNzY1pcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "693fc55d-91b0-44a8-c180-359543ae85bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Insert value for A1 (between 0 and 1): \n",
            "0.8\n",
            "Insert value for A2 (between 0 and 1): \n",
            "0.5\n",
            "\n",
            "Transformed Input is as follow.\n",
            "A1 = {(H, 0.6000000000000001); (M, 0.3999999999999999); (L, 0.0)}\n",
            "A2 = {(H, 0.0); (M, 1.0); (L, 0.0)}\n",
            "\n",
            "Matching degrees of the rules are as follow.\n",
            "Matching Degree of Rule 1 = 0.0\n",
            "Matching Degree of Rule 2 = 0.6000000000000001\n",
            "Matching Degree of Rule 3 = 0.0\n",
            "Matching Degree of Rule 4 = 0.0\n",
            "Matching Degree of Rule 5 = 0.3999999999999999\n",
            "Matching Degree of Rule 6 = 0.0\n",
            "Matching Degree of Rule 7 = 0.0\n",
            "Matching Degree of Rule 8 = 0.0\n",
            "Matching Degree of Rule 9 = 0.0\n",
            "\n",
            "Activation Weights of the rules are as follow.\n",
            "Activation weight of Rule 1 = 0.0\n",
            "Activation weight of Rule 2 = 0.6000000000000001\n",
            "Activation weight of Rule 3 = 0.0\n",
            "Activation weight of Rule 4 = 0.0\n",
            "Activation weight of Rule 5 = 0.3999999999999999\n",
            "Activation weight of Rule 6 = 0.0\n",
            "Activation weight of Rule 7 = 0.0\n",
            "Activation weight of Rule 8 = 0.0\n",
            "Activation weight of Rule 9 = 0.0\n",
            "\n",
            "No upgradation of belief degree required.\n",
            "ER Aggregated Belief Degree for High: 0.0\n",
            "ER Aggregated Belief Degree for Medium: 0.4672897196261681\n",
            "ER Aggregated Belief Degree for Low: 0.2523364485981309\n",
            "Unassigned Degree of Belief: 0.28037383177570097\n",
            "Maximum expected utility: 0.514018691588785\n",
            "Minimum expected utility: 0.23364485981308405\n",
            "Average expected utility: 0.3738317757009345\n",
            "Revised ER Aggregated Belief Degrees considering degree of Incompleteness are as follows:\n",
            "Revised ER Aggregated Belief Degree for High: 0.0\n",
            "Revised ER Aggregated Belief Degree for Medium: 0.6493506493506492\n",
            "Revised ER Aggregated Belief Degree for Low: 0.35064935064935077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fedarated Learning"
      ],
      "metadata": {
        "id": "eWvRUxBVes4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python '/content/drive/MyDrive/FLCode/mainBRB.py'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo7fOCIBgVho",
        "outputId": "fba26702-bbc8-46b1-8a1e-47602fc958c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Insert value for A1 (between 0 and 1): \n",
            "\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the zipfile module\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# loading the temp.zip and creating a zip object\n",
        "with ZipFile(\"/content/drive/MyDrive/FLCode/archive.zip\", 'r') as zObject:\n",
        "\n",
        "\t# Extracting all the members of the zip\n",
        "\t# into a specific location.\n",
        "\tzObject.extractall(path=\"MNISTDataset/\")\n"
      ],
      "metadata": {
        "id": "jf0FX46fibME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone 'https://github.com/stijani/tutorial.git'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BIY-iJii2N7",
        "outputId": "73efe45a-ad32-4ffe-defb-166836aa33ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tutorial'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 7 (delta 0), reused 2 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (7/7), 4.92 KiB | 4.92 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "from imutils import paths\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from fl_mnist_implementation_tutorial_utils import *\n",
        "from mainBRB import *"
      ],
      "metadata": {
        "id": "QjmbO3mWe1Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load(paths, verbose=-1):\n",
        "    '''expects images for each class in seperate dir,\n",
        "    e.g all digits in 0 class in the directory named 0 '''\n",
        "    data = list()\n",
        "    labels = list()\n",
        "    # loop over the input images\n",
        "    for (i, imgpath) in enumerate(paths):\n",
        "        # load the image and extract the class labels\n",
        "        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE)\n",
        "        image = np.array(im_gray).flatten()\n",
        "        label = imgpath.split(os.path.sep)[-2]\n",
        "        # scale the image to [0, 1] and add to list\n",
        "        data.append(image/255)\n",
        "        labels.append(label)\n",
        "        # show an update every `verbose` images\n",
        "        if verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
        "            print(\"[INFO] processed {}/{}\".format(i + 1, len(paths)))\n",
        "    # return a tuple of the data and labels\n",
        "    return data, labels"
      ],
      "metadata": {
        "id": "KLSIkNmGfHE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#declear path to your mnist data folder\n",
        "img_path = '/content/MNISTDataset/trainingSet/trainingSet'\n",
        "\n",
        "#get the path list using the path object\n",
        "image_paths = list(paths.list_images(img_path))\n",
        "\n",
        "#apply our function\n",
        "image_list, label_list = load(image_paths, verbose=10000)\n",
        "\n",
        "#binarize the labels\n",
        "lb = LabelBinarizer()\n",
        "label_list = lb.fit_transform(label_list)\n",
        "\n",
        "#split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(image_list,\n",
        "                                                    label_list,\n",
        "                                                    test_size=0.1,\n",
        "                                                    random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NeV3T7ZfK1L",
        "outputId": "0fb9610a-4f91-45a6-f88a-9d43b792be1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] processed 10000/42000\n",
            "[INFO] processed 20000/42000\n",
            "[INFO] processed 30000/42000\n",
            "[INFO] processed 40000/42000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_clients(image_list, label_list, num_clients=10, initial='clients'):\n",
        "    ''' return: a dictionary with keys clients' names and value as\n",
        "                data shards - tuple of images and label lists.\n",
        "        args:\n",
        "            image_list: a list of numpy arrays of training images\n",
        "            label_list:a list of binarized labels for each image\n",
        "            num_client: number of fedrated members (clients)\n",
        "            initials: the clients'name prefix, e.g, clients_1\n",
        "\n",
        "    '''\n",
        "\n",
        "    #create a list of client names\n",
        "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
        "\n",
        "    #randomize the data\n",
        "    data = list(zip(image_list, label_list))\n",
        "    random.shuffle(data)\n",
        "\n",
        "    #shard data and place at each client\n",
        "    size = len(data)//num_clients\n",
        "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
        "\n",
        "    #number of clients must equal number of shards\n",
        "    assert(len(shards) == len(client_names))\n",
        "\n",
        "    return {client_names[i] : shards[i] for i in range(len(client_names))}"
      ],
      "metadata": {
        "id": "suIxwkwxfPEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create clients\n",
        "clients = create_clients(X_train, y_train, num_clients=10, initial='client')"
      ],
      "metadata": {
        "id": "0XLw0_B1fT8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_data(data_shard, bs=32):\n",
        "    '''Takes in a clients data shard and create a tfds object off it\n",
        "    args:\n",
        "        shard: a data, label constituting a client's data shard\n",
        "        bs:batch size\n",
        "    return:\n",
        "        tfds object'''\n",
        "    #seperate shard into data and labels lists\n",
        "    data, label = zip(*data_shard)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
        "    return dataset.shuffle(len(label)).batch(bs)"
      ],
      "metadata": {
        "id": "-6R4EGv8fXcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#process and batch the training data for each client\n",
        "clients_batched = dict()\n",
        "for (client_name, data) in clients.items():\n",
        "    clients_batched[client_name] = batch_data(data)\n",
        "    print(client_name)\n",
        "    print(list(clients_batched[client_name]))\n",
        "    print(list(batch_data(data)))\n",
        "#process and batch the test set\n",
        "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"
      ],
      "metadata": {
        "id": "6hAmR0CbfejQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleMLP:\n",
        "    @staticmethod\n",
        "    def build(shape, classes):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(200, input_shape=(shape,)))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Dense(200))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Dense(classes))\n",
        "        model.add(Activation(\"softmax\"))\n",
        "        return model"
      ],
      "metadata": {
        "id": "IUYBJVP8ffYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "comms_round = 100\n",
        "#decay=learning_rate/comms_round\n",
        "loss='categorical_crossentropy'\n",
        "metrics = ['accuracy']\n",
        "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=learning_rate,\n",
        "                decay=learning_rate/comms_round,\n",
        "                momentum=0.9\n",
        "               )"
      ],
      "metadata": {
        "id": "GIjMQPxvfiqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_scalling_factor(clients_trn_data, client_name):\n",
        "    client_names = list(clients_trn_data.keys())\n",
        "    #get the bs\n",
        "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
        "    #first calculate the total training data points across clinets\n",
        "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
        "    # get the total number of data points held by a client\n",
        "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
        "    return local_count/global_count\n",
        "\n",
        "\n",
        "def scale_model_weights(weight, scalar):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "    for i in range(steps):\n",
        "        weight_final.append(scalar * weight[i])\n",
        "    return weight_final\n",
        "\n",
        "\n",
        "\n",
        "def sum_scaled_weights(scaled_weight_list):\n",
        "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
        "    avg_grad = list()\n",
        "    #get the average grad accross all client gradients\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "\n",
        "    return avg_grad\n",
        "\n",
        "\n",
        "def test_model(X_test, Y_test,  model, comm_round):\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    #logits = model.predict(X_test, batch_size=100)\n",
        "    logits = model.predict(X_test)\n",
        "    loss = cce(Y_test, logits)\n",
        "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
        "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
        "    return acc, loss"
      ],
      "metadata": {
        "id": "5vy858yNfmIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize global model\n",
        "smlp_global = SimpleMLP()\n",
        "global_model = smlp_global.build(784, 10)\n",
        "\n",
        "#commence global training loop\n",
        "for comm_round in range(comms_round):\n",
        "\n",
        "    # get the global model's weights - will serve as the initial weights for all local models\n",
        "    global_weights = global_model.get_weights()\n",
        "\n",
        "    #initial list to collect local model weights after scalling\n",
        "    scaled_local_weight_list = list()\n",
        "\n",
        "    #randomize client data - using keys\n",
        "    client_names= list(clients_batched.keys())\n",
        "    random.shuffle(client_names)\n",
        "\n",
        "    #loop through each client and create new local model\n",
        "    '''for client in client_names:\n",
        "        smlp_local = SimpleMLP()\n",
        "        local_model = smlp_local.build(784, 10)\n",
        "        local_model.compile(loss=loss,\n",
        "                      optimizer=optimizer,\n",
        "                      metrics=metrics)\n",
        "\n",
        "        #set local model weight to the weight of the global model\n",
        "        local_model.set_weights(global_weights)\n",
        "\n",
        "        #fit local model with client's data\n",
        "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
        "\n",
        "        #scale the model weights and add to list\n",
        "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
        "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "\n",
        "        #clear session to free memory after each communication round\n",
        "        K.clear_session()'''\n",
        "    # Integration of BRB\n",
        "    for client in client_names:\n",
        "        smlp_local = SimpleMLP()\n",
        "        local_model = smlp_local.build(784, 10)\n",
        "        local_model.compile(loss=loss,\n",
        "                      optimizer=optimizer,\n",
        "                      metrics=metrics)\n",
        "\n",
        "        #set local model weight to the weight of the global model\n",
        "        local_model.set_weights(global_weights)\n",
        "\n",
        "        #updated_attribute_weights = calculate_updated_attribute_weights(clients_batched[client])  # Assuming client data is used for BRB calculation\n",
        "        #apply_updated_attribute_weights(local_model, updated_attribute_weights)  # Apply updated attribute weights to the local model\n",
        "        print(f'client: {client}')\n",
        "        print(f'client_batched: {clients_batched[client]}')\n",
        "        #fit local model with client's data\n",
        "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
        "\n",
        "        #scale the model weights and add to list\n",
        "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
        "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "\n",
        "        #clear session to free memory after each communication round\n",
        "        K.clear_session()\n",
        "\n",
        "\n",
        "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
        "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "\n",
        "    #update global model\n",
        "    global_model.set_weights(average_weights)\n",
        "\n",
        "    #test global model and print out metrics after each communications round\n",
        "    for(X_test, Y_test) in test_batched:\n",
        "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "t3gB_MhEfrjQ",
        "outputId": "d9c56d09-c0ce-4f57-a9a9-318eaa8c797f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'SimpleMLP' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bc133fe28065>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#initialize global model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msmlp_global\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mglobal_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmlp_global\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#commence global training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SimpleMLP' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(X_test, Y_test,  model, comm_round):\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    #logits = model.predict(X_test, batch_size=100)\n",
        "    logits = model.predict(X_test)\n",
        "    loss = cce(Y_test, logits)\n",
        "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
        "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
        "    return acc, loss"
      ],
      "metadata": {
        "id": "8TBQtqVQfxMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SGD_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(y_train)).batch(320)\n",
        "smlp_SGD = SimpleMLP()\n",
        "SGD_model = smlp_SGD.build(784, 10)\n",
        "\n",
        "SGD_model.compile(loss=loss,\n",
        "              optimizer=optimizer,\n",
        "              metrics=metrics)\n",
        "\n",
        "# fit the SGD training data to model\n",
        "_ = SGD_model.fit(SGD_dataset, epochs=100, verbose=0)\n",
        "\n",
        "#test the SGD global model and print out metrics\n",
        "for(X_test, Y_test) in test_batched:\n",
        "        SGD_acc, SGD_loss = test_model(X_test, Y_test, SGD_model, 1)"
      ],
      "metadata": {
        "id": "sQZ3uoXkf1k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def non_iid_x(image_list, label_list, x=1, num_intraclass_clients=10):\n",
        "        ''' creates x non_IID clients\n",
        "        args:\n",
        "            image_list: python list of images or data points\n",
        "            label_list: python list of labels\n",
        "            x: none IID severity, 1 means each client will only have one class of data\n",
        "            num_intraclass_client: number of sub-client to be created from each none IID class,\n",
        "            e.g for x=1, we could create 10 further clients by splitting each class into 10\n",
        "\n",
        "        return - dictionary\n",
        "            keys - clients's name,\n",
        "            value - client's non iid 1 data shard (as tuple list of images and labels) '''\n",
        "\n",
        "        non_iid_x_clients = dict()\n",
        "\n",
        "        #create unique label list and shuffle\n",
        "        unique_labels = np.unique(np.array(label_list))\n",
        "        random.shuffle(unique_labels)\n",
        "\n",
        "        #create sub label lists based on x\n",
        "        sub_lab_list = [unique_labels[i:i + x] for i in range(0, len(unique_labels), x)]\n",
        "\n",
        "        for item in sub_lab_list:\n",
        "            class_data = [(image, label) for (image, label) in zip(image_list, label_list) if label in item]\n",
        "\n",
        "            #decouple tuple list into seperate image and label lists\n",
        "            images, labels = zip(*class_data)\n",
        "\n",
        "            # create formated client initials\n",
        "            initial = ''\n",
        "            for lab in item:\n",
        "                initial = initial + lab + '_'\n",
        "\n",
        "            #create num_intraclass_clients clients from the class\n",
        "            intraclass_clients = create_clients(list(images), list(labels), num_intraclass_clients, initial)\n",
        "\n",
        "            #append intraclass clients to main clients'dict\n",
        "            non_iid_x_clients.update(intraclass_clients)\n",
        "\n",
        "        return non_iid_x_clients"
      ],
      "metadata": {
        "id": "SSKm0lAof8Yu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}